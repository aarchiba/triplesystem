{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import cPickle as pickle\n",
    "from glob import glob\n",
    "import subprocess\n",
    "import sys\n",
    "import string\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "import scipy.linalg\n",
    "\n",
    "import threebody\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "#logger.setLevel(logging.DEBUG)\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.debug(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basic_params = dict(files=\"tuned2_aligned_pulses_selected\",\n",
    "                     tzrmjd_middle='auto',\n",
    "                     parfile=\"0337_de430.par\",\n",
    "                     fit_pos=True,\n",
    "                     fit_pm=True,\n",
    "                     fit_px=True,\n",
    "                     efac=1,\n",
    "                     t2_astrometry=True,\n",
    "                     kopeikin=False,\n",
    "                     ppn_mode='heavysimple',\n",
    "                     linear_jumps=True,\n",
    "                     linear_dm=False,\n",
    "                     fdn_range=(1,5),\n",
    "                     priors=('dbeta','dgamma'),\n",
    "                     toa_mode=\"pipeline\",\n",
    "                     dmx_span=365.2425/2,\n",
    "                     variable_dm=True,\n",
    "                     variable_ipm=True)\n",
    "F = threebody.Fitter(**basic_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical marginalization correction\n",
    "\n",
    "When we're doing MCMC, we actually simplify the problem by handling the linear parts separately. Specifically, for each set of orbital parameters, we do a linear least-squares fit for all the linear parts and return the log-likelihood of the best-fit value. For this to be proper MCMC, we need to think of this as analytically marginalizing over the possible linear-part values. This requires us to add a correction to the log-probability, which is an integral over all linear-part values. If the fit matrix is $A$, then this correction term is $\\log(\\det(A^TA))$. Currently we compute this by multplying out $A^TA$ and calling the numpy function `slogdet`. This doesn't operate in long double, the product $A^TA$ has a worse condition number than $A$ itself, and we know the condition number of $A$ can be improved by rescaling its columns. So let's try to do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just a check: let's re-derive the analytical marginalization.\n",
    "\n",
    "Our problem is parameterized by a pair of vectors $(x,y)$, and the log-likelihood of the data $d$ given $(x,y)$ is calculated using $L(x,y)=\\log(|F(x)y-d|^2/2)$. But we are only really interested in the distribution of $x$, so we want to construct an $L(x)$ that gives rise to the same posterior distribution of $x$ values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the calculation\n",
    "\n",
    "If we have a matrix $A$, the SVD gives us $A=USV^T$, and the eigenvalues of $A^TA$ are the squares of the diagonal values of $S$. So we can simplify the calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 8.2541645702244448)\n",
      "8.25416457022\n"
     ]
    }
   ],
   "source": [
    "A = np.random.randn(10,4).astype(np.longdouble)\n",
    "\n",
    "s = scipy.linalg.svdvals(A)\n",
    "\n",
    "print(np.linalg.slogdet(np.dot(A.T,A).astype(np.float)))\n",
    "print(np.sum(np.log(s))*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A, names = F.compute_linear_matrix()\n",
    "A /= F.phase_uncerts[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 537.75505864568834)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.slogdet(np.dot(A.T,A).astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to rescale the columns of $A$ so that they all have the same $L^2$ norm. (It's not clear that this is necessarily the best rescaling but it does seem to help.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537.75505529991886988"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales = np.sum(A**2,axis=0)\n",
    "As = A/scales[None,:]\n",
    "(np.linalg.slogdet(np.dot(As.T,As).astype(float))[1]\n",
    " +2*np.sum(np.log(scales)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537.75103079184101434"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = scipy.linalg.svdvals(As)\n",
    "2*np.sum(np.log(scales)+np.log(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Kepler improvements\n",
    "\n",
    "Currently the Keplerian model that translates from orbital elements to positions and masses uses `scipy.optimize.newton` in a few places. Even apart for whatever inaccuracies it has, this only uses double precision. Can we do better easily?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_from_eccentric(e, ecc):\n",
    "    return ecc-e*np.sin(ecc)\n",
    "def eccentric_from_mean(e, mean_anomaly):\n",
    "    \"\"\"Compute the eccentric anomaly from the mean anomaly                                                             \n",
    "                                                                                                                       \n",
    "    Inputs:                                                                                                            \n",
    "        e - the eccentricity                                                                                           \n",
    "        mean_anomaly - the mean anomaly                                                                                \n",
    "                                                                                                                       \n",
    "    Outputs:                                                                                                           \n",
    "        eccentric_anomaly - the true anomaly                                                                           \n",
    "        derivatives - pair of derivatives with respect to the two inputs                                               \n",
    "    \"\"\"\n",
    "    eccentric_anomaly = newton(\n",
    "            lambda E: E-e*np.sin(E)-mean_anomaly,\n",
    "            mean_anomaly,\n",
    "            lambda E: 1-e*np.cos(E),\n",
    "            tol=1e-20)\n",
    "    eccentric_anomaly_de = np.sin(eccentric_anomaly)/(1-e*np.cos(eccentric_anomaly))\n",
    "    eccentric_anomaly_prime = (1-e*np.cos(eccentric_anomaly))**(-1)\n",
    "    return eccentric_anomaly, [eccentric_anomaly_de, eccentric_anomaly_prime]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0210176124166828374, 0.0, dtype('float128'), dtype('float128'))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = np.arctan(np.longdouble(1))*1.3\n",
    "e = np.longdouble(5e-1)\n",
    "ecc = eccentric_from_mean(e, m)[0]\n",
    "r = mean_from_eccentric(e,ecc)\n",
    "r, (r-m)/m, ecc.dtype, r.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
